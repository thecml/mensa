{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This demo shows how to train the MENSA model on single-event, competing risks and multi-event data. The demo uses synthetic data for better reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd party\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import config as cfg\n",
    "import torch\n",
    "import random\n",
    "import warnings\n",
    "import argparse\n",
    "import os\n",
    "from SurvivalEVAL.Evaluator import LifelinesEvaluator\n",
    "\n",
    "# Local\n",
    "from data_loader import (SingleEventSyntheticDataLoader,\n",
    "                         CompetingRiskSyntheticDataLoader,\n",
    "                         MultiEventSyntheticDataLoader)\n",
    "from utility.survival import (make_time_bins, convert_to_structured,\n",
    "                              compute_l1_difference, predict_survival_function)\n",
    "from utility.data import dotdict\n",
    "from utility.config import load_config\n",
    "from utility.evaluation import global_C_index, local_C_index\n",
    "from mensa.model import MENSA\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\".*The 'nopython' keyword.*\")\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "# Setup precision\n",
    "dtype = torch.float64\n",
    "torch.set_default_dtype(dtype)\n",
    "\n",
    "# Setup device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Synthetic single-event\n",
    "\n",
    "We generate a synthetic single-event (K=2) dataset from a Weibull DGP with no dependence (k_tau=0) and linear risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load synthetic data for single-event case\n",
    "data_config = load_config(cfg.DGP_CONFIGS_DIR, f\"synthetic_se.yaml\")\n",
    "dl = SingleEventSyntheticDataLoader().load_data(data_config=data_config,\n",
    "                                                linear=True, copula_name=\"\",\n",
    "                                                k_tau=0, device=device, dtype=dtype)\n",
    "train_dict, valid_dict, test_dict = dl.split_data(train_size=0.7, valid_size=0.1, test_size=0.2,\n",
    "                                                  random_state=0)\n",
    "n_features = train_dict['X'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch  149/1000]:  15%|█▍        | 148/1000 [00:48<04:40,  3.04it/s, Training loss = 2.4492, Validation loss = 2.4631]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at iteration 148, best valid loss: 2.459958369040445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Make time bins\n",
    "time_bins = make_time_bins(train_dict['T'], event=None, dtype=dtype).to(device)\n",
    "time_bins = torch.cat((torch.tensor([0]).to(device), time_bins))\n",
    "\n",
    "# Load model configuration and make model\n",
    "config = load_config(cfg.MENSA_CONFIGS_DIR, f\"synthetic.yaml\")\n",
    "n_epochs = config['n_epochs']\n",
    "lr = config['lr']\n",
    "batch_size = config['batch_size']\n",
    "layers = config['layers']\n",
    "model = MENSA(n_features, layers=layers, n_events=2, device=device)\n",
    "lr_dict = {'network': lr}\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dict, valid_dict, optimizer='adam', verbose=True,\n",
    "          n_epochs=n_epochs, patience=10, batch_size=batch_size, lr_dict=lr_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E1: [0.6698866820771986, 0.1028138761033593, 1.3681644611295425, 1.6488549808110948, 1.7436583083292598, 0.5193912618049151]\n"
     ]
    }
   ],
   "source": [
    "# Get predictions for the single event\n",
    "model_preds = model.predict(test_dict['X'].to(device), time_bins, risk=0)\n",
    "model_preds = pd.DataFrame(model_preds, columns=time_bins.cpu().numpy())\n",
    "\n",
    "# Use SurvivalEVAL package to calculate popular metrics\n",
    "y_train_time = train_dict['T']\n",
    "y_train_event = (train_dict['E'])*1.0\n",
    "y_test_time = test_dict['T']\n",
    "y_test_event = (test_dict['E'])*1.0\n",
    "lifelines_eval = LifelinesEvaluator(model_preds.T, y_test_time, y_test_event,\n",
    "                                    y_train_time, y_train_event)\n",
    "\n",
    "ci = lifelines_eval.concordance()[0]\n",
    "ibs = lifelines_eval.integrated_brier_score()\n",
    "mae_hinge = lifelines_eval.mae(method=\"Hinge\")\n",
    "mae_margin = lifelines_eval.mae(method=\"Margin\")\n",
    "mae_pseudo = lifelines_eval.mae(method=\"Pseudo_obs\")\n",
    "d_calib = lifelines_eval.d_calibration()[0]\n",
    "\n",
    "metrics = [ci, ibs, mae_hinge, mae_margin, mae_pseudo, d_calib]\n",
    "print(\"E1: \" + str(metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Synthetic competing risks\n",
    "\n",
    "We generate a synthetic competing risks (K=3) dataset from a Weibull DGP with no dependence (k_tau=0) and linear risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load synthetic data for competing risks case\n",
    "data_config = load_config(cfg.DGP_CONFIGS_DIR, f\"synthetic_cr.yaml\")\n",
    "dl = CompetingRiskSyntheticDataLoader().load_data(data_config, k_tau=0, copula_name=\"\",\n",
    "                                                  linear=True, device=device, dtype=dtype)\n",
    "train_dict, valid_dict, test_dict = dl.split_data(train_size=0.7, valid_size=0.1, test_size=0.2,\n",
    "                                                  random_state=0)\n",
    "n_features = train_dict['X'].shape[1]\n",
    "n_events = dl.n_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch   83/1000]:   8%|▊         | 82/1000 [02:20<26:15,  1.72s/it, Training loss = 2.9794, Validation loss = 3.0258]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at iteration 82, best valid loss: 3.0250137994749773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Make time bins\n",
    "time_bins = make_time_bins(train_dict['T'], event=None, dtype=dtype).to(device)\n",
    "time_bins = torch.cat((torch.tensor([0]).to(device), time_bins))\n",
    "\n",
    "# Load model configuration and make model\n",
    "config = load_config(cfg.MENSA_CONFIGS_DIR, f\"synthetic.yaml\")\n",
    "n_epochs = config['n_epochs']\n",
    "lr = config['lr']\n",
    "batch_size = config['batch_size']\n",
    "layers = config['layers']\n",
    "model = MENSA(n_features, layers=layers, n_events=3, device=device)\n",
    "lr_dict = {'network': lr}\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dict, valid_dict, optimizer='adam', verbose=True,\n",
    "          n_epochs=n_epochs, patience=10, batch_size=batch_size, lr_dict=lr_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E1: [0.6451154845105632, 0.05835831089749532, 1.7810479189557282, 5.166257262716706e-105, 0.6266510637837749, 0.6771743258640335]\n",
      "E2: [0.6169488622155539, 0.08223018811164312, 2.4177082183863488, 0.0, 0.6266510637837749, 0.6771743258640335]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions for competing risks\n",
    "all_preds = []\n",
    "for i in range(n_events):\n",
    "    model_preds = model.predict(test_dict['X'].to(device), time_bins, risk=i+1)\n",
    "    model_preds = pd.DataFrame(model_preds, columns=time_bins.cpu().numpy())\n",
    "    all_preds.append(model_preds)\n",
    "    \n",
    "# Calculate local and global CI\n",
    "y_test_time = np.stack([test_dict['T'] for _ in range(n_events)], axis=1)\n",
    "y_test_event = np.stack([np.array((test_dict['E'] == i+1)*1.0) for i in range(n_events)], axis=1)\n",
    "all_preds_arr = [df.to_numpy() for df in all_preds]\n",
    "global_ci = global_C_index(all_preds_arr, y_test_time, y_test_event)\n",
    "local_ci = local_C_index(all_preds_arr, y_test_time, y_test_event)\n",
    "\n",
    "# Make evaluation for each event\n",
    "for event_id, surv_preds in enumerate(all_preds):\n",
    "    n_train_samples = len(train_dict['X'])\n",
    "    n_test_samples= len(test_dict['X'])\n",
    "    y_train_time = train_dict[f'T{event_id+1}']\n",
    "    y_train_event = np.array([1] * n_train_samples)\n",
    "    y_test_time = test_dict[f'T{event_id+1}']\n",
    "    y_test_event = np.array([1] * n_test_samples)\n",
    "    lifelines_eval = LifelinesEvaluator(surv_preds.T, y_test_time, y_test_event,\n",
    "                                        y_train_time, y_train_event)\n",
    "    \n",
    "    ci =  lifelines_eval.concordance()[0]\n",
    "    ibs = lifelines_eval.integrated_brier_score(num_points=len(time_bins))\n",
    "    mae = lifelines_eval.mae(method='Uncensored')\n",
    "    d_calib = lifelines_eval.d_calibration()[0]\n",
    "    \n",
    "    metrics = [ci, ibs, mae, d_calib, global_ci, local_ci]\n",
    "    print(f'E{event_id+1}: ' + f'{metrics}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Synthetic multi-event\n",
    "\n",
    "We generate a synthetic multi-event (K=3) dataset from a Weibull DGP with no dependence (k_tau=0) and linear risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split data\n",
    "data_config = load_config(cfg.DGP_CONFIGS_DIR, f\"synthetic_me.yaml\")\n",
    "dl = MultiEventSyntheticDataLoader().load_data(data_config, k_taus=[0, 0, 0], copula_names=[],\n",
    "                                               linear=True, device=device, dtype=dtype)\n",
    "train_dict, valid_dict, test_dict = dl.split_data(train_size=0.7, valid_size=0.1, test_size=0.2,\n",
    "                                                  random_state=0)\n",
    "n_features = train_dict['X'].shape[1]\n",
    "n_events = dl.n_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch   71/1000]:   7%|▋         | 70/1000 [02:36<34:42,  2.24s/it, Training loss = 5.4547, Validation loss = 5.5427]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at iteration 70, best valid loss: 5.54063224715259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Make time bins\n",
    "time_bins = make_time_bins(train_dict['T'], event=None, dtype=dtype).to(device)\n",
    "time_bins = torch.cat((torch.tensor([0]).to(device), time_bins))\n",
    "\n",
    "# Load model configuration and make model\n",
    "config = load_config(cfg.MENSA_CONFIGS_DIR, f\"synthetic.yaml\")\n",
    "n_epochs = config['n_epochs']\n",
    "lr = config['lr']\n",
    "batch_size = config['batch_size']\n",
    "layers = config['layers']\n",
    "model = MENSA(n_features, layers=layers, n_events=3, device=device)\n",
    "lr_dict = {'network': lr}\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dict, valid_dict, optimizer='adam', verbose=True,\n",
    "          n_epochs=n_epochs, patience=10, batch_size=batch_size, lr_dict=lr_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E1: [0.6437182971777223, 0.09675261740522718, 1.496732421395548, 6.168807708841666e-174, 0.6373376724813675, 0.6449748275219094]\n",
      "E2: [0.6408499633529109, 0.1030200286345988, 1.5152797329605348, 4.8233355062202835e-54, 0.6373376724813675, 0.6449748275219094]\n",
      "E3: [0.6258876777920004, 0.08720759817958376, 1.7690591442505441, 0.0, 0.6373376724813675, 0.6449748275219094]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions for multi-event\n",
    "all_preds = []\n",
    "for i in range(n_events):\n",
    "    model_preds = model.predict(test_dict['X'].to(device), time_bins, risk=i)\n",
    "    model_preds = pd.DataFrame(model_preds, columns=time_bins.cpu().numpy())\n",
    "    all_preds.append(model_preds)\n",
    "\n",
    "# Calculate local and global CI\n",
    "all_preds_arr = [df.to_numpy() for df in all_preds]\n",
    "global_ci = global_C_index(all_preds_arr, test_dict['T'].numpy(), test_dict['E'].numpy())\n",
    "local_ci = local_C_index(all_preds_arr, test_dict['T'].numpy(), test_dict['E'].numpy())\n",
    "\n",
    "# Make evaluation for each event\n",
    "for event_id, surv_preds in enumerate(all_preds):\n",
    "    n_train_samples = len(train_dict['X'])\n",
    "    n_test_samples= len(test_dict['X'])\n",
    "    y_train_time = train_dict['T'][:,event_id]\n",
    "    y_train_event = np.array([1] * n_train_samples)\n",
    "    y_test_time = test_dict['T'][:,event_id]\n",
    "    y_test_event = np.array([1] * n_test_samples)\n",
    "    lifelines_eval = LifelinesEvaluator(surv_preds.T, y_test_time, y_test_event,\n",
    "                                        y_train_time, y_train_event)\n",
    "    \n",
    "    ci =  lifelines_eval.concordance()[0]\n",
    "    ibs = lifelines_eval.integrated_brier_score(num_points=len(time_bins))\n",
    "    mae = lifelines_eval.mae(method='Uncensored')\n",
    "    d_calib = lifelines_eval.d_calibration()[0]\n",
    "    \n",
    "    metrics = [ci, ibs, mae, d_calib, global_ci, local_ci]\n",
    "    print(f'E{event_id+1}: ' + f'{metrics}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39-mensa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
