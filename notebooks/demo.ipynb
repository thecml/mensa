{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "MENSA is a deep learning model that jointly models flexible time-to-event distributions for multiple events, whether competing or co-occurring. The data format is as follows:\n",
    "\n",
    "For single-event: X = [x_1, x_2, ... x_n], T = [t_1, t_2, ..., t_i], E = [e_1, e_2, ... e_i]\\\n",
    "For competing risks: X = [x_1, x_2, ... x_n], T = [t_1, t_2, ..., t_i], E = [e_1, e_2, ... e_i]\\\n",
    "For multi-event: X = [x_1, x_2, ... x_n], T = [[t_i1, t_i2, ..., t_ik], ...], E = [[e_i1, e_i2, ..., e_ik], ...]\n",
    "\n",
    "Here, $n$ is the number of covariates, $i$ is the subject and $k$ denotes the number of events.\\\n",
    "The demo uses a synthetic data generator (DGP) for better reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cml\\miniconda3\\envs\\py39-mensa\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 3rd party\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import config as cfg\n",
    "import torch\n",
    "import random\n",
    "from SurvivalEVAL.Evaluator import LifelinesEvaluator\n",
    "\n",
    "# Local\n",
    "from data_loader import (SingleEventSyntheticDataLoader,\n",
    "                         CompetingRiskSyntheticDataLoader,\n",
    "                         MultiEventSyntheticDataLoader)\n",
    "from utility.survival import make_time_bins\n",
    "from utility.config import load_config\n",
    "from utility.evaluation import global_C_index, local_C_index\n",
    "from mensa.model import MENSA\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "# Setup precision\n",
    "dtype = torch.float32\n",
    "torch.set_default_dtype(dtype)\n",
    "\n",
    "# Setup device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single-event prediction\n",
    "\n",
    "We generate a synthetic single-event dataset from a linear Weibull DGP.\\\n",
    "This generates one event and a censoring event.\n",
    "\n",
    "See the concrete implementation for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load synthetic data for single-event case\n",
    "data_config = load_config(cfg.DGP_CONFIGS_DIR, f\"synthetic_se.yaml\")\n",
    "dl = SingleEventSyntheticDataLoader().load_data(data_config=data_config,\n",
    "                                                linear=True, copula_name=\"\",\n",
    "                                                k_tau=0, device=device, dtype=dtype)\n",
    "train_dict, valid_dict, test_dict = dl.split_data(train_size=0.7, valid_size=0.1, test_size=0.2,\n",
    "                                                  random_state=0)\n",
    "n_features = train_dict['X'].shape[1]\n",
    "n_events = dl.n_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch   65/1000]:   6%|▋         | 64/1000 [02:09<31:32,  2.02s/it, Training loss = 1.6314, Validation loss = 1.6341]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at iteration 64, best valid loss: 1.6266281269490719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Make time bins\n",
    "time_bins = make_time_bins(train_dict['T'], event=None, dtype=dtype).to(device)\n",
    "time_bins = torch.cat((torch.tensor([0]).to(device), time_bins))\n",
    "\n",
    "# Define the model\n",
    "config = load_config(cfg.MENSA_CONFIGS_DIR, f\"synthetic.yaml\")\n",
    "n_epochs = config['n_epochs']\n",
    "n_dists = config['n_dists']\n",
    "lr = config['lr']\n",
    "batch_size = config['batch_size']\n",
    "layers = config['layers']\n",
    "weight_decay = config['weight_decay']\n",
    "dropout_rate = config['dropout_rate']\n",
    "model = MENSA(n_features, layers=layers, dropout_rate=dropout_rate,\n",
    "              n_events=n_events, n_dists=n_dists, device=device)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dict, valid_dict, learning_rate=lr, n_epochs=n_epochs,\n",
    "          weight_decay=weight_decay, patience=20,\n",
    "          batch_size=batch_size, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event 1: [0.8264203032510422, 0.14102957628774732, 0.7430959016409477, 4.057748994853522, 8.675915396351094, 0.9919812897555466]\n"
     ]
    }
   ],
   "source": [
    "# Get predictions for the single event\n",
    "model_preds = model.predict(test_dict['X'].to(device), time_bins, risk=1)\n",
    "model_preds = pd.DataFrame(model_preds, columns=time_bins.cpu().numpy())\n",
    "\n",
    "# Use SurvivalEVAL package to calculate popular metrics\n",
    "y_train_time = train_dict['T']\n",
    "y_train_event = (train_dict['E'])*1.0\n",
    "y_test_time = test_dict['T']\n",
    "y_test_event = (test_dict['E'])*1.0\n",
    "lifelines_eval = LifelinesEvaluator(model_preds.T, y_test_time, y_test_event,\n",
    "                                    y_train_time, y_train_event)\n",
    "\n",
    "ci = lifelines_eval.concordance()[0]\n",
    "ibs = lifelines_eval.integrated_brier_score(num_points=len(time_bins))\n",
    "mae_hinge = lifelines_eval.mae(method=\"Hinge\")\n",
    "mae_margin = lifelines_eval.mae(method=\"Margin\")\n",
    "mae_pseudo = lifelines_eval.mae(method=\"Pseudo_obs\")\n",
    "d_calib = lifelines_eval.d_calibration()[0]\n",
    "\n",
    "metrics = [ci, ibs, mae_hinge, mae_margin, mae_pseudo, d_calib]\n",
    "print(\"Event 1: \" + str(metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competing-risks prediction\n",
    "\n",
    "We generate a synthetic competing risks (K=2) dataset from a linear Weibull DGP.\\\n",
    "This generates two actual competing events and a censoring event.\n",
    "\n",
    "See the concrete implementation for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load synthetic data for competing risks case\n",
    "data_config = load_config(cfg.DGP_CONFIGS_DIR, f\"synthetic_cr.yaml\")\n",
    "dl = CompetingRiskSyntheticDataLoader().load_data(data_config, k_tau=0, copula_name=\"\",\n",
    "                                                  linear=True, device=device, dtype=dtype)\n",
    "train_dict, valid_dict, test_dict = dl.split_data(train_size=0.7, valid_size=0.1, test_size=0.2,\n",
    "                                                  random_state=0)\n",
    "n_features = train_dict['X'].shape[1]\n",
    "n_events = dl.n_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch   53/1000]:   5%|▌         | 52/1000 [02:28<45:06,  2.86s/it, Training loss = 3.0400, Validation loss = 3.0619] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at iteration 52, best valid loss: 3.037816122174263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Make time bins\n",
    "time_bins = make_time_bins(train_dict['T'], event=None, dtype=dtype).to(device)\n",
    "time_bins = torch.cat((torch.tensor([0]).to(device), time_bins))\n",
    "\n",
    "# Define the model\n",
    "config = load_config(cfg.MENSA_CONFIGS_DIR, f\"synthetic.yaml\")\n",
    "n_epochs = config['n_epochs']\n",
    "n_dists = config['n_dists']\n",
    "lr = config['lr']\n",
    "batch_size = config['batch_size']\n",
    "layers = config['layers']\n",
    "weight_decay = config['weight_decay']\n",
    "dropout_rate = config['dropout_rate']\n",
    "model = MENSA(n_features, layers=layers, dropout_rate=dropout_rate,\n",
    "              n_events=n_events, n_dists=n_dists, device=device)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dict, valid_dict, learning_rate=lr, n_epochs=n_epochs,\n",
    "          weight_decay=weight_decay, patience=20,\n",
    "          batch_size=batch_size, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event 1: [0.8043287834385147, 0.05231590006247374, 2.309306383357845, 3.02639914112114, 3.5447934641750756, 0.8050830495826518, 0.5, 1.160900001417501e-48]\n",
      "Event 2: [0.8050663704787273, 0.05061734354618599, 2.227111067519106, 2.9408706216240192, 3.4670379015696806, 0.8050830495826518, 0.5, 8.201324032156303e-41]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions for competing risks\n",
    "all_preds = []\n",
    "for i in range(n_events):\n",
    "    model_preds = model.predict(test_dict['X'].to(device), time_bins, risk=i+1) # skip censoring event\n",
    "    model_preds = pd.DataFrame(model_preds, columns=time_bins.cpu().numpy())\n",
    "    all_preds.append(model_preds)\n",
    "    \n",
    "# Calculate local and global CI\n",
    "y_test_time = np.stack([test_dict['T'] for _ in range(n_events)], axis=1)\n",
    "y_test_event = np.stack([np.array((test_dict['E'] == i+1)*1.0) for i in range(n_events)], axis=1)\n",
    "all_preds_arr = [df.to_numpy() for df in all_preds]\n",
    "global_ci = global_C_index(all_preds_arr, y_test_time, y_test_event)\n",
    "local_ci = local_C_index(all_preds_arr, y_test_time, y_test_event)\n",
    "\n",
    "# Use SurvivalEVAL package to calculate popular metrics\n",
    "for event_id, surv_preds in enumerate(all_preds):\n",
    "    n_train_samples = len(train_dict['X'])\n",
    "    n_test_samples= len(test_dict['X'])\n",
    "    y_train_time = train_dict['T']\n",
    "    y_train_event = (train_dict['E'])*1.0\n",
    "    y_test_time = test_dict['T']\n",
    "    y_test_event = (test_dict['E'])*1.0\n",
    "    \n",
    "    lifelines_eval = LifelinesEvaluator(surv_preds.T, y_test_time, y_test_event,\n",
    "                                        y_train_time, y_train_event)\n",
    "    \n",
    "    ci =  lifelines_eval.concordance()[0]\n",
    "    ibs = lifelines_eval.integrated_brier_score(num_points=len(time_bins))\n",
    "    mae_hinge = lifelines_eval.mae(method=\"Hinge\")\n",
    "    mae_margin = lifelines_eval.mae(method=\"Margin\")\n",
    "    mae_pseudo = lifelines_eval.mae(method=\"Pseudo_obs\")\n",
    "    d_calib = lifelines_eval.d_calibration()[0]\n",
    "    \n",
    "    metrics = [ci, ibs, mae_hinge, mae_margin, mae_pseudo, global_ci, local_ci, d_calib]\n",
    "    print(f'Event {event_id+1}: ' + f'{metrics}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-event prediction\n",
    "\n",
    "We generate a synthetic multi-event dataset from a linaer Weibull DGP.\\\n",
    "This generates four events that are not mutually exclusive.\n",
    "\n",
    "See the concrete implementation for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split data\n",
    "data_config = load_config(cfg.DGP_CONFIGS_DIR, f\"synthetic_me.yaml\")\n",
    "dl = MultiEventSyntheticDataLoader().load_data(data_config, k_taus=[0, 0, 0, 0], copula_names=[],\n",
    "                                               linear=True, device=device, dtype=dtype)\n",
    "train_dict, valid_dict, test_dict = dl.split_data(train_size=0.7, valid_size=0.1, test_size=0.2,\n",
    "                                                  random_state=0)\n",
    "n_features = train_dict['X'].shape[1]\n",
    "n_events = dl.n_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch   53/1000]:   5%|▌         | 52/1000 [03:13<58:43,  3.72s/it, Training loss = 1.0020, Validation loss = 1.1343]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at iteration 52, best valid loss: 1.1037923842668533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Make time bins\n",
    "time_bins = make_time_bins(train_dict['T'], event=None, dtype=dtype).to(device)\n",
    "time_bins = torch.cat((torch.tensor([0]).to(device), time_bins))\n",
    "\n",
    "# Define the model\n",
    "config = load_config(cfg.MENSA_CONFIGS_DIR, f\"synthetic.yaml\")\n",
    "n_epochs = config['n_epochs']\n",
    "n_dists = config['n_dists']\n",
    "lr = config['lr']\n",
    "batch_size = config['batch_size']\n",
    "layers = config['layers']\n",
    "weight_decay = config['weight_decay']\n",
    "dropout_rate = config['dropout_rate']\n",
    "model = MENSA(n_features, layers=layers, dropout_rate=dropout_rate,\n",
    "              n_events=n_events, n_dists=n_dists, device=device)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dict, valid_dict, learning_rate=lr, n_epochs=n_epochs,\n",
    "          weight_decay=weight_decay, patience=20,\n",
    "          batch_size=batch_size, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event 1: [0.5455040220418196, 0.10427038499911316, 3.2357458968808084, 3.246547542048791, 3.2573994274212748, 0.6113270885405223, 0.9583267059010657, 5.881630695454767e-51]\n",
      "Event 2: [0.8654050593507538, 0.005448711502378367, 9.722952147316132, 436954.7114455938, 436878.8745664984, 0.6113270885405223, 0.9583267059010657, 0.999999736778676]\n",
      "Event 3: [0.8770241386520456, 0.005398366164207229, 6.828169383327557, 54796.13677355559, 54713.190123198525, 0.6113270885405223, 0.9583267059010657, 0.9999352277056895]\n",
      "Event 4: [0.84814308118389, 0.0063197004172375094, 26.005477683199928, 116247.00586593169, 116177.28249421371, 0.6113270885405223, 0.9583267059010657, 0.9998765266341876]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions for multi-event\n",
    "all_preds = []\n",
    "for i in range(n_events):\n",
    "    model_preds = model.predict(test_dict['X'].to(device), time_bins, risk=i+1) # skip censoring event\n",
    "    model_preds = pd.DataFrame(model_preds, columns=time_bins.cpu().numpy())\n",
    "    all_preds.append(model_preds)\n",
    "\n",
    "# Calculate local and global CI\n",
    "all_preds_arr = [df.to_numpy() for df in all_preds]\n",
    "global_ci = global_C_index(all_preds_arr, test_dict['T'].numpy(), test_dict['E'].numpy())\n",
    "local_ci = local_C_index(all_preds_arr, test_dict['T'].numpy(), test_dict['E'].numpy())\n",
    "\n",
    "# Use SurvivalEVAL package to calculate popular metrics\n",
    "for event_id, surv_preds in enumerate(all_preds):\n",
    "    n_train_samples = len(train_dict['X'])\n",
    "    n_test_samples= len(test_dict['X'])\n",
    "    y_train_time = train_dict['T'][:,event_id]\n",
    "    y_train_event = train_dict['E'][:,event_id]\n",
    "    y_test_time = test_dict['T'][:,event_id]\n",
    "    y_test_event = test_dict['E'][:,event_id]\n",
    "    \n",
    "    lifelines_eval = LifelinesEvaluator(surv_preds.T, y_test_time, y_test_event,\n",
    "                                        y_train_time, y_train_event)\n",
    "    \n",
    "    ci =  lifelines_eval.concordance()[0]\n",
    "    ibs = lifelines_eval.integrated_brier_score(num_points=len(time_bins))\n",
    "    mae_hinge = lifelines_eval.mae(method=\"Hinge\")\n",
    "    mae_margin = lifelines_eval.mae(method=\"Margin\")\n",
    "    mae_pseudo = lifelines_eval.mae(method=\"Pseudo_obs\")\n",
    "    d_calib = lifelines_eval.d_calibration()[0]\n",
    "    \n",
    "    metrics = [ci, ibs, mae_hinge, mae_margin, mae_pseudo, global_ci, local_ci, d_calib]\n",
    "    print(f'Event {event_id+1}: ' + f'{metrics}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39-mensa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
