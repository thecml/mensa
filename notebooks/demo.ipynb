{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Given an instance, a multi-event survival model predicts the time until that instance experiences each of several different events. These events are not mutually exclusive and there are often statistical dependencies between them. MENSA works by jointly learning the K event distributions as a convex combination of Weibull distributions. This approach leverages mutual information between events that may be lost in models that assume independence.\n",
    "\n",
    "The data format is as follows:\n",
    "\n",
    "For single-event: X = [x_1, x_2, ... x_n], T = [t_1, t_2, ..., t_i], E = [e_1, e_2, ... e_i]\\\n",
    "For competing risks: X = [x_1, x_2, ... x_n], T = [t_1, t_2, ..., t_i], E = [e_1, e_2, ... e_i]\\\n",
    "For multi-event: X = [x_1, x_2, ... x_n], T = [[t_i1, t_i2, ..., t_ik], ...], E = [[e_i1, e_i2, ..., e_ik], ...]\n",
    "\n",
    "Here, $n$ is the number of covariates, $i$ is the subject and $k$ denotes the number of events.\\\n",
    "The demo uses a synthetic data generator (DGP) for better reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd party\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import config as cfg\n",
    "import torch\n",
    "import random\n",
    "from SurvivalEVAL.Evaluator import LifelinesEvaluator\n",
    "\n",
    "# Local\n",
    "from data_loader import (SingleEventSyntheticDataLoader,\n",
    "                         CompetingRiskSyntheticDataLoader,\n",
    "                         MultiEventSyntheticDataLoader)\n",
    "from utility.survival import make_time_bins\n",
    "from utility.config import load_config\n",
    "from utility.evaluation import global_C_index, local_C_index\n",
    "from mensa.model import MENSA\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "# Setup precision\n",
    "dtype = torch.float64\n",
    "torch.set_default_dtype(dtype)\n",
    "\n",
    "# Setup device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-event prediction\n",
    "\n",
    "We generate a synthetic single-event dataset from a Weibull DGP with no dependence (k_tau=0) and linear risk.\\\n",
    "This generates one event and a censoring event.\n",
    "\n",
    "See the concrete implementation for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load synthetic data for single-event case\n",
    "data_config = load_config(cfg.DGP_CONFIGS_DIR, f\"synthetic_se.yaml\")\n",
    "dl = SingleEventSyntheticDataLoader().load_data(data_config=data_config,\n",
    "                                                linear=True, copula_name=\"\",\n",
    "                                                k_tau=0, device=device, dtype=dtype)\n",
    "train_dict, valid_dict, test_dict = dl.split_data(train_size=0.7, valid_size=0.1, test_size=0.2,\n",
    "                                                  random_state=0)\n",
    "n_features = train_dict['X'].shape[1]\n",
    "n_events = dl.n_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch   44/1000]:   4%|▍         | 43/1000 [00:51<19:09,  1.20s/it, Training loss = 2.8303, Validation loss = 2.9180]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at iteration 43, best valid loss: 2.9059269795695584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Make time bins\n",
    "time_bins = make_time_bins(train_dict['T'], event=None, dtype=dtype).to(device)\n",
    "time_bins = torch.cat((torch.tensor([0]).to(device), time_bins))\n",
    "\n",
    "# Define the model\n",
    "config = load_config(cfg.MENSA_CONFIGS_DIR, f\"synthetic.yaml\")\n",
    "n_epochs = config['n_epochs']\n",
    "n_dists = config['n_dists']\n",
    "lr = config['lr']\n",
    "batch_size = config['batch_size']\n",
    "layers = config['layers']\n",
    "weight_decay = config['weight_decay']\n",
    "dropout_rate = config['dropout_rate']\n",
    "model = MENSA(n_features, layers=layers, dropout_rate=dropout_rate,\n",
    "                n_events=n_events, n_dists=n_dists, device=device)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dict, valid_dict, learning_rate=lr, n_epochs=n_epochs,\n",
    "          weight_decay=weight_decay, patience=10,\n",
    "          batch_size=batch_size, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event 1: [0.820584836947118, 0.13181309379379055, 1.7259265116239435, 3.366090142802187, 6.369244971546377, 0.27877795442251696]\n"
     ]
    }
   ],
   "source": [
    "# Get predictions for the single event\n",
    "model_preds = model.predict(test_dict['X'].to(device), time_bins, risk=1)\n",
    "model_preds = pd.DataFrame(model_preds, columns=time_bins.cpu().numpy())\n",
    "\n",
    "# Use SurvivalEVAL package to calculate popular metrics\n",
    "y_train_time = train_dict['T']\n",
    "y_train_event = (train_dict['E'])*1.0\n",
    "y_test_time = test_dict['T']\n",
    "y_test_event = (test_dict['E'])*1.0\n",
    "lifelines_eval = LifelinesEvaluator(model_preds.T, y_test_time, y_test_event,\n",
    "                                    y_train_time, y_train_event)\n",
    "\n",
    "ci = lifelines_eval.concordance()[0]\n",
    "ibs = lifelines_eval.integrated_brier_score(num_points=len(time_bins))\n",
    "mae_hinge = lifelines_eval.mae(method=\"Hinge\")\n",
    "mae_margin = lifelines_eval.mae(method=\"Margin\")\n",
    "mae_pseudo = lifelines_eval.mae(method=\"Pseudo_obs\")\n",
    "d_calib = lifelines_eval.d_calibration()[0]\n",
    "\n",
    "metrics = [ci, ibs, mae_hinge, mae_margin, mae_pseudo, d_calib]\n",
    "print(\"Event 1: \" + str(metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Competing-risks prediction\n",
    "\n",
    "We generate a synthetic competing risks (K=2) dataset from a Weibull DGP with no dependence (k_tau=0) and linear risk.\\\n",
    "This generates two actual competing events and a censoring event.\n",
    "\n",
    "See the concrete implementation for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load synthetic data for competing risks case\n",
    "data_config = load_config(cfg.DGP_CONFIGS_DIR, f\"synthetic_cr.yaml\")\n",
    "dl = CompetingRiskSyntheticDataLoader().load_data(data_config, k_tau=0, copula_name=\"\",\n",
    "                                                  linear=True, device=device, dtype=dtype)\n",
    "train_dict, valid_dict, test_dict = dl.split_data(train_size=0.7, valid_size=0.1, test_size=0.2,\n",
    "                                                  random_state=0)\n",
    "n_features = train_dict['X'].shape[1]\n",
    "n_events = dl.n_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch   65/1000]:   6%|▋         | 64/1000 [01:32<22:27,  1.44s/it, Training loss = 1.9101, Validation loss = 1.8936]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at iteration 64, best valid loss: 1.888639416781049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Make time bins\n",
    "time_bins = make_time_bins(train_dict['T'], event=None, dtype=dtype).to(device)\n",
    "time_bins = torch.cat((torch.tensor([0]).to(device), time_bins))\n",
    "\n",
    "# Define the model\n",
    "config = load_config(cfg.MENSA_CONFIGS_DIR, f\"synthetic.yaml\")\n",
    "n_epochs = config['n_epochs']\n",
    "n_dists = config['n_dists']\n",
    "lr = config['lr']\n",
    "batch_size = config['batch_size']\n",
    "layers = config['layers']\n",
    "weight_decay = config['weight_decay']\n",
    "dropout_rate = config['dropout_rate']\n",
    "model = MENSA(n_features, layers=layers, dropout_rate=dropout_rate,\n",
    "              n_events=n_events, n_dists=n_dists, device=device)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dict, valid_dict, learning_rate=lr, n_epochs=n_epochs,\n",
    "          weight_decay=weight_decay, patience=10,\n",
    "          batch_size=batch_size, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event 1: [0.6542917324907846, 0.053425780883329096, 0.06376680407782936, 5.308637727519044, 4.3206685346010785, 0.6661400737230121, 0.5384615384615384, 0.9999999999927013]\n",
      "Event 2: [0.6614007372301212, 0.05114556084876717, 0.07307959210563922, 4.965387994837668, 3.9657969006068923, 0.6661400737230121, 0.5384615384615384, 0.999999995179888]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions for competing risks\n",
    "all_preds = []\n",
    "for i in range(n_events):\n",
    "    model_preds = model.predict(test_dict['X'].to(device), time_bins, risk=i+1) # skip censoring event\n",
    "    model_preds = pd.DataFrame(model_preds, columns=time_bins.cpu().numpy())\n",
    "    all_preds.append(model_preds)\n",
    "    \n",
    "# Calculate local and global CI\n",
    "y_test_time = np.stack([test_dict['T'] for _ in range(n_events)], axis=1)\n",
    "y_test_event = np.stack([np.array((test_dict['E'] == i+1)*1.0) for i in range(n_events)], axis=1)\n",
    "all_preds_arr = [df.to_numpy() for df in all_preds]\n",
    "global_ci = global_C_index(all_preds_arr, y_test_time, y_test_event)\n",
    "local_ci = local_C_index(all_preds_arr, y_test_time, y_test_event)\n",
    "\n",
    "# Use SurvivalEVAL package to calculate popular metrics\n",
    "for event_id, surv_preds in enumerate(all_preds):\n",
    "    n_train_samples = len(train_dict['X'])\n",
    "    n_test_samples= len(test_dict['X'])\n",
    "    y_train_time = train_dict['T']\n",
    "    y_train_event = (train_dict['E'])*1.0\n",
    "    y_test_time = test_dict['T']\n",
    "    y_test_event = (test_dict['E'])*1.0\n",
    "    \n",
    "    lifelines_eval = LifelinesEvaluator(surv_preds.T, y_test_time, y_test_event,\n",
    "                                        y_train_time, y_train_event)\n",
    "    \n",
    "    ci =  lifelines_eval.concordance()[0]\n",
    "    ibs = lifelines_eval.integrated_brier_score(num_points=len(time_bins))\n",
    "    mae_hinge = lifelines_eval.mae(method=\"Hinge\")\n",
    "    mae_margin = lifelines_eval.mae(method=\"Margin\")\n",
    "    mae_pseudo = lifelines_eval.mae(method=\"Pseudo_obs\")\n",
    "    d_calib = lifelines_eval.d_calibration()[0]\n",
    "    \n",
    "    metrics = [ci, ibs, mae_hinge, mae_margin, mae_pseudo, global_ci, local_ci, d_calib]\n",
    "    print(f'Event {event_id+1}: ' + f'{metrics}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-event prediction\n",
    "\n",
    "We generate a synthetic multi-event dataset from a Weibull DGP with no dependence (k_tau=0) and linear risk.\\\n",
    "This generates four events that are not mutually exclusive.\n",
    "\n",
    "See the concrete implementation for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split data\n",
    "data_config = load_config(cfg.DGP_CONFIGS_DIR, f\"synthetic_me.yaml\")\n",
    "dl = MultiEventSyntheticDataLoader().load_data(data_config, k_taus=[0, 0, 0, 0], copula_names=[],\n",
    "                                               linear=True, device=device, dtype=dtype)\n",
    "train_dict, valid_dict, test_dict = dl.split_data(train_size=0.7, valid_size=0.1, test_size=0.2,\n",
    "                                                  random_state=0)\n",
    "n_features = train_dict['X'].shape[1]\n",
    "n_events = dl.n_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch   23/1000]:   2%|▏         | 22/1000 [00:43<32:15,  1.98s/it, Training loss = 4.8831, Validation loss = 4.6399] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at iteration 22, best valid loss: 4.1192310296907495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Make time bins\n",
    "time_bins = make_time_bins(train_dict['T'], event=None, dtype=dtype).to(device)\n",
    "time_bins = torch.cat((torch.tensor([0]).to(device), time_bins))\n",
    "\n",
    "# Define the model\n",
    "config = load_config(cfg.MENSA_CONFIGS_DIR, f\"synthetic.yaml\")\n",
    "n_epochs = config['n_epochs']\n",
    "n_dists = config['n_dists']\n",
    "lr = config['lr']\n",
    "batch_size = config['batch_size']\n",
    "layers = config['layers']\n",
    "weight_decay = config['weight_decay']\n",
    "dropout_rate = config['dropout_rate']\n",
    "model = MENSA(n_features, layers=layers, dropout_rate=dropout_rate,\n",
    "              n_events=n_events, n_dists=n_dists, device=device)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dict, valid_dict, learning_rate=lr, n_epochs=n_epochs,\n",
    "          weight_decay=weight_decay, patience=10,\n",
    "          batch_size=batch_size, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event 1: [0.8165445961316854, 0.058125933917466725, 3.909632098450562, 52.77514858680799, 44.61149061882991, 0.8245981936541406, 0.7030947775628626, 0.0031317579165855564]\n",
      "Event 2: [0.8294284223861689, 0.0349372619218568, 4.387488412974439, 52.8410442064724, 48.90438511547283, 0.8245981936541406, 0.7030947775628626, 0.1447498868804959]\n",
      "Event 3: [0.8238387670420866, 0.03402523956982753, 4.842466609322007, 61.50006834710062, 57.42028850610096, 0.8245981936541406, 0.7030947775628626, 0.41379230771236253]\n",
      "Event 4: [0.8294652823883908, 0.035361355580257, 4.430020968922511, 62.14436841138883, 58.36433422241283, 0.8245981936541406, 0.7030947775628626, 0.08479103958546916]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions for multi-event\n",
    "all_preds = []\n",
    "for i in range(n_events):\n",
    "    model_preds = model.predict(test_dict['X'].to(device), time_bins, risk=i+1) # skip censoring event\n",
    "    model_preds = pd.DataFrame(model_preds, columns=time_bins.cpu().numpy())\n",
    "    all_preds.append(model_preds)\n",
    "\n",
    "# Calculate local and global CI\n",
    "all_preds_arr = [df.to_numpy() for df in all_preds]\n",
    "global_ci = global_C_index(all_preds_arr, test_dict['T'].numpy(), test_dict['E'].numpy())\n",
    "local_ci = local_C_index(all_preds_arr, test_dict['T'].numpy(), test_dict['E'].numpy())\n",
    "\n",
    "# Use SurvivalEVAL package to calculate popular metrics\n",
    "for event_id, surv_preds in enumerate(all_preds):\n",
    "    n_train_samples = len(train_dict['X'])\n",
    "    n_test_samples= len(test_dict['X'])\n",
    "    y_train_time = train_dict['T'][:,event_id]\n",
    "    y_train_event = train_dict['E'][:,event_id]\n",
    "    y_test_time = test_dict['T'][:,event_id]\n",
    "    y_test_event = test_dict['E'][:,event_id]\n",
    "    \n",
    "    lifelines_eval = LifelinesEvaluator(surv_preds.T, y_test_time, y_test_event,\n",
    "                                        y_train_time, y_train_event)\n",
    "    \n",
    "    ci =  lifelines_eval.concordance()[0]\n",
    "    ibs = lifelines_eval.integrated_brier_score(num_points=len(time_bins))\n",
    "    mae_hinge = lifelines_eval.mae(method=\"Hinge\")\n",
    "    mae_margin = lifelines_eval.mae(method=\"Margin\")\n",
    "    mae_pseudo = lifelines_eval.mae(method=\"Pseudo_obs\")\n",
    "    d_calib = lifelines_eval.d_calibration()[0]\n",
    "    \n",
    "    metrics = [ci, ibs, mae_hinge, mae_margin, mae_pseudo, global_ci, local_ci, d_calib]\n",
    "    print(f'Event {event_id+1}: ' + f'{metrics}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39-mensa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
