{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Given an instance, a multi-event survival model predicts the time until that instance experiences each of several different events. These events are not mutually exclusive and there are often statistical dependencies between them. MENSA works by jointly learning the K event distributions as a convex combination of Weibull distributions. This approach leverages mutual information between events that may be lost in models that assume independence.\n",
    "\n",
    "The data format is as follows:\n",
    "\n",
    "For single-event: X = [x_1, x_2, ... x_n], T = [t_1, t_2, ..., t_i], E = [e_1, e_2, ... e_i]\\\n",
    "For competing risks: X = [x_1, x_2, ... x_n], T = [t_1, t_2, ..., t_i], E = [e_1, e_2, ... e_i]\\\n",
    "For multi-event: X = [x_1, x_2, ... x_n], T = [[t_i1, t_i2, ..., t_ik], ...], E = [[e_i1, e_i2, ..., e_ik], ...]\n",
    "\n",
    "Here, $n$ is the number of covariates, $i$ is the subject and $k$ denotes the number of events.\\\n",
    "The demo uses a synthetic data generator (DGP) for better reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\au475271\\Miniconda3\\envs\\py39-mensa\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 3rd party\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import config as cfg\n",
    "import torch\n",
    "import random\n",
    "from SurvivalEVAL.Evaluator import LifelinesEvaluator\n",
    "\n",
    "# Local\n",
    "from data_loader import (SingleEventSyntheticDataLoader,\n",
    "                         CompetingRiskSyntheticDataLoader,\n",
    "                         MultiEventSyntheticDataLoader)\n",
    "from utility.survival import make_time_bins\n",
    "from utility.config import load_config\n",
    "from utility.evaluation import global_C_index, local_C_index\n",
    "from mensa.model import MENSA\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "# Setup precision\n",
    "dtype = torch.float64\n",
    "torch.set_default_dtype(dtype)\n",
    "\n",
    "# Setup device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Synthetic single-event\n",
    "\n",
    "We generate a synthetic single-event dataset from a Weibull DGP with no dependence (k_tau=0) and linear risk.\\\n",
    "This generates one actual event (E1) and a censoring event, so K=2.\n",
    "\n",
    "See the concrete implementation for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load synthetic data for single-event case\n",
    "data_config = load_config(cfg.DGP_CONFIGS_DIR, f\"synthetic_se.yaml\")\n",
    "dl = SingleEventSyntheticDataLoader().load_data(data_config=data_config,\n",
    "                                                linear=True, copula_name=\"\",\n",
    "                                                k_tau=0, device=device, dtype=dtype)\n",
    "train_dict, valid_dict, test_dict = dl.split_data(train_size=0.7, valid_size=0.1, test_size=0.2,\n",
    "                                                  random_state=0)\n",
    "n_features = train_dict['X'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch   25/1000]:   2%|▏         | 24/1000 [00:14<09:31,  1.71it/s, Training loss = 2.7674, Validation loss = 2.7579]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at iteration 24, best valid loss: 2.7574658100447373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Make time bins\n",
    "time_bins = make_time_bins(train_dict['T'], event=None, dtype=dtype).to(device)\n",
    "time_bins = torch.cat((torch.tensor([0]).to(device), time_bins))\n",
    "\n",
    "# Define the model\n",
    "n_epochs = 1000\n",
    "lr = 0.001\n",
    "batch_size = 32\n",
    "layers = [32]\n",
    "model = MENSA(n_features, layers=layers, n_events=2, device=device)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dict, valid_dict, optimizer='adam', n_epochs=n_epochs,\n",
    "          verbose=True, patience=10, batch_size=batch_size, learning_rate=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E1: [0.6438241344402896, 0.0827682825874046, 1.4229945782198683, 2.0510194644112607, 2.366014410048409, 0.6404361712054161]\n"
     ]
    }
   ],
   "source": [
    "# Get predictions for the single event\n",
    "model_preds = model.predict(test_dict['X'].to(device), time_bins, risk=0)\n",
    "model_preds = pd.DataFrame(model_preds, columns=time_bins.cpu().numpy())\n",
    "\n",
    "# Use SurvivalEVAL package to calculate popular metrics\n",
    "y_train_time = train_dict['T']\n",
    "y_train_event = (train_dict['E'])*1.0\n",
    "y_test_time = test_dict['T']\n",
    "y_test_event = (test_dict['E'])*1.0\n",
    "lifelines_eval = LifelinesEvaluator(model_preds.T, y_test_time, y_test_event,\n",
    "                                    y_train_time, y_train_event)\n",
    "\n",
    "ci = lifelines_eval.concordance()[0]\n",
    "ibs = lifelines_eval.integrated_brier_score(num_points=len(time_bins))\n",
    "mae_hinge = lifelines_eval.mae(method=\"Hinge\")\n",
    "mae_margin = lifelines_eval.mae(method=\"Margin\")\n",
    "mae_pseudo = lifelines_eval.mae(method=\"Pseudo_obs\")\n",
    "d_calib = lifelines_eval.d_calibration()[0]\n",
    "\n",
    "metrics = [ci, ibs, mae_hinge, mae_margin, mae_pseudo, d_calib]\n",
    "print(\"E1: \" + str(metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Synthetic competing risks\n",
    "\n",
    "We generate a synthetic competing risks (K=3) dataset from a Weibull DGP with no dependence (k_tau=0) and linear risk.\\\n",
    "This generates two actual competing events (E1 and E2) and a censoring event, so K=3.\n",
    "\n",
    "See the concrete implementation for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load synthetic data for competing risks case\n",
    "data_config = load_config(cfg.DGP_CONFIGS_DIR, f\"synthetic_cr.yaml\")\n",
    "dl = CompetingRiskSyntheticDataLoader().load_data(data_config, k_tau=0, copula_name=\"\",\n",
    "                                                  linear=True, device=device, dtype=dtype)\n",
    "train_dict, valid_dict, test_dict = dl.split_data(train_size=0.7, valid_size=0.1, test_size=0.2,\n",
    "                                                  random_state=0)\n",
    "n_features = train_dict['X'].shape[1]\n",
    "n_events = dl.n_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch   33/1000]:   3%|▎         | 32/1000 [01:36<48:42,  3.02s/it, Training loss = 3.0576, Validation loss = 3.0684]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at iteration 32, best valid loss: 3.0583668206534242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Make time bins\n",
    "time_bins = make_time_bins(train_dict['T'], event=None, dtype=dtype).to(device)\n",
    "time_bins = torch.cat((torch.tensor([0]).to(device), time_bins))\n",
    "\n",
    "# Define the model\n",
    "config = load_config(cfg.MENSA_CONFIGS_DIR, f\"synthetic.yaml\")\n",
    "n_epochs = 1000\n",
    "lr = 0.001\n",
    "batch_size = 32\n",
    "layers = [32]\n",
    "model = MENSA(n_features, layers=layers, n_events=3, device=device)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dict, valid_dict, optimizer='adam', verbose=True,\n",
    "          n_epochs=n_epochs, patience=10, batch_size=batch_size, learning_rate=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E1: [0.6233223899146121, 0.07862104796271611, 1.453079890217305, 1.8323388584045197, 2.0044932498816443, 0.6310468593107317, 0.7427236794825728, 0.07817974543059338]\n",
      "E2: [0.6030719043844883, 0.1270125648583262, 2.357923647837968, 2.6548876807282413, 2.786660651046932, 0.6310468593107317, 0.7427236794825728, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions for competing risks\n",
    "all_preds = []\n",
    "for i in range(n_events):\n",
    "    model_preds = model.predict(test_dict['X'].to(device), time_bins, risk=i+1) # skip censoring event\n",
    "    model_preds = pd.DataFrame(model_preds, columns=time_bins.cpu().numpy())\n",
    "    all_preds.append(model_preds)\n",
    "    \n",
    "# Calculate local and global CI\n",
    "y_test_time = np.stack([test_dict['T'] for _ in range(n_events)], axis=1)\n",
    "y_test_event = np.stack([np.array((test_dict['E'] == i+1)*1.0) for i in range(n_events)], axis=1)\n",
    "all_preds_arr = [df.to_numpy() for df in all_preds]\n",
    "global_ci = global_C_index(all_preds_arr, y_test_time, y_test_event)\n",
    "local_ci = local_C_index(all_preds_arr, y_test_time, y_test_event)\n",
    "\n",
    "# Use SurvivalEVAL package to calculate popular metrics\n",
    "for event_id, surv_preds in enumerate(all_preds):\n",
    "    n_train_samples = len(train_dict['X'])\n",
    "    n_test_samples= len(test_dict['X'])\n",
    "    y_train_time = train_dict['T']\n",
    "    y_train_event = (train_dict['E'])*1.0\n",
    "    y_test_time = test_dict['T']\n",
    "    y_test_event = (test_dict['E'])*1.0\n",
    "    \n",
    "    lifelines_eval = LifelinesEvaluator(surv_preds.T, y_test_time, y_test_event,\n",
    "                                        y_train_time, y_train_event)\n",
    "    \n",
    "    ci =  lifelines_eval.concordance()[0]\n",
    "    ibs = lifelines_eval.integrated_brier_score(num_points=len(time_bins))\n",
    "    mae_hinge = lifelines_eval.mae(method=\"Hinge\")\n",
    "    mae_margin = lifelines_eval.mae(method=\"Margin\")\n",
    "    mae_pseudo = lifelines_eval.mae(method=\"Pseudo_obs\")\n",
    "    d_calib = lifelines_eval.d_calibration()[0]\n",
    "    \n",
    "    metrics = [ci, ibs, mae_hinge, mae_margin, mae_pseudo, global_ci, local_ci, d_calib]\n",
    "    print(f'E{event_id+1}: ' + f'{metrics}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Synthetic multi-event\n",
    "\n",
    "We generate a synthetic multi-event dataset from a Weibull DGP with no dependence (k_tau=0) and linear risk.\\\n",
    "This generates three actual events (E1, E2 and E3) that are not mutually exclusive.\n",
    "\n",
    "See the concrete implementation for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split data\n",
    "data_config = load_config(cfg.DGP_CONFIGS_DIR, f\"synthetic_me.yaml\")\n",
    "dl = MultiEventSyntheticDataLoader().load_data(data_config, k_taus=[0, 0, 0], copula_names=[],\n",
    "                                               linear=True, device=device, dtype=dtype)\n",
    "train_dict, valid_dict, test_dict = dl.split_data(train_size=0.7, valid_size=0.1, test_size=0.2,\n",
    "                                                  random_state=0)\n",
    "n_features = train_dict['X'].shape[1]\n",
    "n_events = dl.n_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch   15/1000]:   1%|▏         | 14/1000 [00:38<44:51,  2.73s/it, Training loss = 5.7725, Validation loss = 5.7746]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at iteration 14, best valid loss: 5.774478964792162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Make time bins\n",
    "time_bins = make_time_bins(train_dict['T'], event=None, dtype=dtype).to(device)\n",
    "time_bins = torch.cat((torch.tensor([0]).to(device), time_bins))\n",
    "\n",
    "# Define the model\n",
    "config = load_config(cfg.MENSA_CONFIGS_DIR, f\"synthetic.yaml\")\n",
    "n_epochs = 1000\n",
    "lr = 0.001\n",
    "batch_size = 32\n",
    "layers = [32]\n",
    "model = MENSA(n_features, layers=layers, n_events=3, device=device)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dict, valid_dict, optimizer='adam', verbose=True,\n",
    "          n_epochs=n_epochs, patience=10, batch_size=batch_size, learning_rate=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E1: [0.6473869482805308, 0.09973084075206892, 1.4692220771505582, 1.789775818310862, 2.1172795637266755, 0.6492982528336503, 0.656479217603912, 0.5174783063076376]\n",
      "E2: [0.6534588275188322, 0.09880397663715398, 1.4844745602293303, 1.92568067659878, 2.3769391677369254, 0.6492982528336503, 0.656479217603912, 0.15314234845264074]\n",
      "E3: [0.6472322127790788, 0.10115955313497431, 1.4556439486280301, 1.480436909290556, 1.505255749855323, 0.6492982528336503, 0.656479217603912, 0.4807914868307198]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions for multi-event\n",
    "all_preds = []\n",
    "for i in range(n_events):\n",
    "    model_preds = model.predict(test_dict['X'].to(device), time_bins, risk=i)\n",
    "    model_preds = pd.DataFrame(model_preds, columns=time_bins.cpu().numpy())\n",
    "    all_preds.append(model_preds)\n",
    "\n",
    "# Calculate local and global CI\n",
    "all_preds_arr = [df.to_numpy() for df in all_preds]\n",
    "global_ci = global_C_index(all_preds_arr, test_dict['T'].numpy(), test_dict['E'].numpy())\n",
    "local_ci = local_C_index(all_preds_arr, test_dict['T'].numpy(), test_dict['E'].numpy())\n",
    "\n",
    "# Use SurvivalEVAL package to calculate popular metrics\n",
    "for event_id, surv_preds in enumerate(all_preds):\n",
    "    n_train_samples = len(train_dict['X'])\n",
    "    n_test_samples= len(test_dict['X'])\n",
    "    y_train_time = train_dict['T'][:,event_id]\n",
    "    y_train_event = train_dict['E'][:,event_id]\n",
    "    y_test_time = test_dict['T'][:,event_id]\n",
    "    y_test_event = test_dict['E'][:,event_id]\n",
    "    \n",
    "    lifelines_eval = LifelinesEvaluator(surv_preds.T, y_test_time, y_test_event,\n",
    "                                        y_train_time, y_train_event)\n",
    "    \n",
    "    ci =  lifelines_eval.concordance()[0]\n",
    "    ibs = lifelines_eval.integrated_brier_score(num_points=len(time_bins))\n",
    "    mae_hinge = lifelines_eval.mae(method=\"Hinge\")\n",
    "    mae_margin = lifelines_eval.mae(method=\"Margin\")\n",
    "    mae_pseudo = lifelines_eval.mae(method=\"Pseudo_obs\")\n",
    "    d_calib = lifelines_eval.d_calibration()[0]\n",
    "    \n",
    "    metrics = [ci, ibs, mae_hinge, mae_margin, mae_pseudo, global_ci, local_ci, d_calib]\n",
    "    print(f'E{event_id+1}: ' + f'{metrics}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39-mensa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
